syntax = "proto3";

package yandex.cloud.priv.mdb.kafka.v1;

import "google/protobuf/field_mask.proto";
import "google/protobuf/timestamp.proto";
import "yandex/cloud/api/operation.proto";
import "yandex/cloud/priv/sensitive.proto";
import "yandex/cloud/priv/validation.proto";
import "yandex/cloud/priv/operation/operation.proto";
import "yandex/cloud/priv/mdb/kafka/v1/cluster.proto";
import "yandex/cloud/priv/mdb/kafka/v1/topic.proto";
import "yandex/cloud/priv/mdb/kafka/v1/user.proto";
import "yandex/cloud/priv/mdb/kafka/v1/maintenance.proto";

option go_package = "a.yandex-team.ru/cloud/bitbucket/private-api/yandex/cloud/priv/mdb/kafka/v1;kafka";
option java_outer_classname = "KFCS";


// A set of methods for managing Apache Kafka Cluster resources.
service ClusterService {
  // Returns the specified Apache Kafka Cluster resource.
  //
  // To get the list of available Apache Kafka Cluster resources, make a [List] request.
  rpc Get (GetClusterRequest) returns (Cluster) {
  }

  // Retrieves the list of Apache Kafka Cluster resources that belong
  // to the specified folder.
  rpc List (ListClustersRequest) returns (ListClustersResponse) {
  }

  // Creates a Apache Kafka cluster in the specified folder.
  rpc Create (CreateClusterRequest) returns (operation.Operation) {
    option (yandex.cloud.api.operation) = {
      metadata: "CreateClusterMetadata"
      response: "Cluster"
    };
  }

  // Updates the specified Apache Kafka cluster.
  rpc Update (UpdateClusterRequest) returns (operation.Operation) {
    option (yandex.cloud.api.operation) = {
      metadata: "UpdateClusterMetadata"
      response: "Cluster"
    };
  }

  // Deletes the specified Apache Kafka cluster.
  rpc Delete (DeleteClusterRequest) returns (operation.Operation) {
    option (yandex.cloud.api.operation) = {
      metadata: "DeleteClusterMetadata"
      response: "google.protobuf.Empty"
    };
  }

  // Moves the specified Apache Kafka cluster to the specified folder.
  rpc Move (MoveClusterRequest) returns (operation.Operation) {
    option (yandex.cloud.api.operation) = {
      metadata: "MoveClusterMetadata"
      response: "Cluster"
    };
  }

  // Start the specified Apache Kafka cluster.
  rpc Start (StartClusterRequest) returns (operation.Operation) {
    option (yandex.cloud.api.operation) = {
      metadata: "StartClusterMetadata"
      response: "Cluster"
    };
  }

  // Stop the specified Apache Kafka cluster.
  rpc Stop (StopClusterRequest) returns (operation.Operation) {
    option (yandex.cloud.api.operation) = {
      metadata: "StopClusterMetadata"
      response: "Cluster"
    };
  }

  // Reschedule planned maintenance operation.
  rpc RescheduleMaintenance (RescheduleMaintenanceRequest) returns (operation.Operation) {
    option (yandex.cloud.api.operation) = {
      metadata: "RescheduleMaintenanceMetadata"
      response: "Cluster"
    };
  }

  // Retrieves logs for the specified Apache Kafka cluster.
  // For more information about logs, see the [Logs](/docs/mdb/kafka/concepts/logs) section in the documentation.
  rpc ListLogs (ListClusterLogsRequest) returns (ListClusterLogsResponse) {
  }

  // Same as ListLogs but using server-side streaming. Also allows for 'tail -f' semantics.
  rpc StreamLogs (StreamClusterLogsRequest) returns (stream StreamLogRecord) {
  }

  // Retrieves the list of Operation resources for the specified cluster.
  rpc ListOperations (ListClusterOperationsRequest) returns (ListClusterOperationsResponse) {
  }

  // Retrieves a list of hosts for the specified cluster.
  rpc ListHosts (ListClusterHostsRequest) returns (ListClusterHostsResponse) {
  }

}

message GetClusterRequest {
  // ID of the Apache Kafka Cluster resource to return.
  // To get the cluster ID use a [ClusterService.List] request.
  string cluster_id = 1 [(required) = true, (length) = "<=50"];
}

message ListClustersRequest {
  // ID of the folder to list Apache Kafka clusters in.
  // To get the folder ID, use a [yandex.cloud.resourcemanager.v1.FolderService.List] request.
  string folder_id = 1 [(required) = true, (length) = "<=50"];

  // The maximum number of results per page to return. If the number of available
  // results is larger than [page_size], the service returns a [ListClustersResponse.next_page_token]
  // that can be used to get the next page of results in subsequent list requests.
  int64 page_size = 2 [(value) = "<=1000"];

  // Page token. To get the next page of results, set [page_token] to the [ListClustersResponse.next_page_token]
  // returned by a previous list request.
  string page_token = 3 [(length) = "<=100"];

  // A filter expression that filters resources listed in the response.
  // The expression must specify:
  // 1. The field name. Currently you can only use filtering with the [Cluster.name] field.
  // 2. An operator. Can be either `=` or `!=` for single values, `IN` or `NOT IN` for lists of values.
  // 3. The value. Must be 1-63 characters long and match the regular expression `^[a-zA-Z0-9_-]+$`.
  string filter = 4 [(length) = "<=1000"];
}

message ListClustersResponse {
  // List of Apache Kafka Cluster resources.
  repeated Cluster clusters = 1;

  // This token allows you to get the next page of results for list requests. If the number of results
  // is larger than [ListClustersRequest.page_size], use the [next_page_token] as the value
  // for the [ListClustersRequest.page_token] parameter in the next list request. Each subsequent
  // list request will have its own [next_page_token] to continue paging through the results.
  string next_page_token = 2;
}

message CreateClusterRequest {
  // ID of the folder to create the Apache Kafka cluster in.
  string folder_id = 1 [(required) = true, (length) = "<=50"];

  // Name of the Apache Kafka cluster. The name must be unique within the folder.
  string name = 2 [(required) = true, (length) = "<=63", (pattern) = "[a-zA-Z0-9_-]*"];

  // Description of the Apache Kafka cluster.
  string description = 3 [(length) = "<=256"];

  // Custom labels for the Apache Kafka cluster as `` key:value `` pairs. Maximum 64 per resource.
  // For example, "project": "mvp" or "source": "dictionary".
  map<string, string> labels = 4 [(priv.size) = "<=64", (length) = "<=63", (pattern) = "[-_0-9a-z]*", (map_key).length = "1-63", (map_key).pattern = "[a-z][-_0-9a-z]*"];

  // Deployment environment of the Apache Kafka cluster.
  Cluster.Environment environment = 5;

  // Configuration and resources for hosts that should be created for the Apache Kafka cluster.
  ConfigSpec config_spec = 6;

  // Descriptions of topics to be created in the Apache Kafka cluster.
  repeated TopicSpec topic_specs = 7;

  // Descriptions of database users to be created in the Apache Kafka cluster.
  repeated UserSpec user_specs = 8;

  // ID of the network to create the cluster in.
  string network_id = 10 [(length) = "<=50"];

  // IDs of subnets to create brokers in.
  repeated string subnet_id = 11;

  // User security groups
  repeated string security_group_ids = 12;

  // Host groups to place VMs of cluster on.
  repeated string host_group_ids = 13;

  // Deletion Protection inhibits deletion of the cluster
  bool deletion_protection = 14;

  // Window of maintenance operations.
  MaintenanceWindow maintenance_window = 15;
}

message CreateClusterMetadata {
  // ID of the Apache Kafka cluster that is being created.
  string cluster_id = 1;
}

message UpdateClusterRequest {
  // ID of the Apache Kafka Cluster resource to update.
  // To get the Apache Kafka cluster ID, use a [ClusterService.List] request.
  string cluster_id = 1 [(required) = true, (length) = "<=50"];

  // Field mask that specifies which fields of the Apache Kafka Cluster resource should be updated.
  google.protobuf.FieldMask update_mask = 2;

  // New description of the Apache Kafka cluster.
  string description = 3 [(length) = "<=256"];

  // Custom labels for the Apache Kafka cluster as `` key:value `` pairs. Maximum 64 per resource.
  // For example, "project": "mvp" or "source": "dictionary".
  //
  // The new set of labels will completely replace the old ones. To add a label, request the current
  // set with the [ClusterService.Get] method, then send an [ClusterService.Update] request with the new label added to the set.
  map<string, string> labels = 4 [(priv.size) = "<=64", (length) = "<=63", (pattern) = "[-_0-9a-z]*", (map_key).length = "1-63", (map_key).pattern = "[a-z][-_0-9a-z]*"];

  // New configuration and resources for hosts in the cluster.
  ConfigSpec config_spec = 5;

  string name = 6 [(length) = "<=63", (pattern) = "[a-zA-Z0-9_-]*"];

  // User security groups
  repeated string security_group_ids = 7;

  // Deletion Protection inhibits deletion of the cluster
  bool deletion_protection = 8;

  // Window of maintenance operations.
  MaintenanceWindow maintenance_window = 9;
}

message UpdateClusterMetadata {
  // ID of the Apache Kafka Cluster resource that is being updated.
  string cluster_id = 1;
}

message DeleteClusterRequest {
  // ID of the Apache Kafka cluster to delete.
  // To get the Apache Kafka cluster ID, use a [ClusterService.List] request.
  string cluster_id = 1 [(required) = true, (length) = "<=50"];
}

message DeleteClusterMetadata {
  // ID of the Apache Kafka cluster that is being deleted.
  string cluster_id = 1;
}

message ListClusterLogsRequest {
  // ID of the Apache Kafka cluster to request logs for.
  // To get the Apache Kafka cluster ID use a [ClusterService.List] request.
  string cluster_id = 1 [(required) = true, (length) = "<=50"];

  // Columns from the logs table to request.
  // If no columns are specified, entire log records are returned.
  repeated string column_filter = 2;

  // Start timestamp for the logs request, in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) text format.
  google.protobuf.Timestamp from_time = 3;

  // End timestamp for the logs request, in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) text format.
  google.protobuf.Timestamp to_time = 4;

  // The maximum number of results per page to return. If the number of available
  // results is larger than [page_size], the service returns a [ListClusterLogsResponse.next_page_token]
  // that can be used to get the next page of results in subsequent list requests.
  int64 page_size = 5 [(value) = "<=1000"];

  // Page token. To get the next page of results, set [page_token] to the
  // [ListClusterLogsResponse.next_page_token] returned by a previous list request.
  string page_token = 6 [(length) = "<=100"];

  // Always return `next_page_token`, even if current page is empty.
  bool always_next_page_token = 7;

  // A filter expression that filters resources listed in the response.
  // The expression must specify:
  // 1. The field name. Currently filtering can be applied to the [LogRecord.logs.message.hostname] field.
  // 2. A conditional operator. Can be either `=` or `!=` for single values, `IN` or `NOT IN` for lists of values.
  // 3. The value. Must be 1-63 characters long and match the regular expression `^[a-z0-9.-]{1,61}$`.
  // Examples of a filter: `message.hostname='node1.db.cloud.yandex.net'`
  string filter = 8 [(length) = "<=1000"];
}

message LogRecord {
  // Log record timestamp in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) text format.
  google.protobuf.Timestamp timestamp = 1;

  // Contents of the log record.
  map<string, string> message = 2;
}

message ListClusterLogsResponse {
  // Requested log records.
  repeated LogRecord logs = 1;

  // This token allows you to get the next page of results for list requests. If the number of results
  // is larger than [ListClusterLogsRequest.page_size], use the [next_page_token] as the value
  // for the [ListClusterLogsRequest.page_token] query parameter in the next list request.
  // Each subsequent list request will have its own [next_page_token] to continue paging through the results.
  // This value is interchangeable with `next_record_token` from StreamLogs method.
  string next_page_token = 2;
}

message StreamLogRecord {
  // One of the requested log records.
  LogRecord record = 1;

  // This token allows you to continue streaming logs starting from the exact
  // same record. To continue streaming, specify value of `next_record_token`
  // as value for `record_token` parameter in the next StreamLogs request.
  // This value is interchangeable with `next_page_token` from ListLogs method.
  string next_record_token = 2 [(sensitive) = false];
}

message StreamClusterLogsRequest {
  // Required. ID of the Kafka cluster.
  string cluster_id = 1 [(required) = true, (length) = "<=50"];

  // Columns from logs table to get in the response.
  repeated string column_filter = 2;

  // Start timestamp for the logs request.
  google.protobuf.Timestamp from_time = 3;

  // End timestamp for the logs request.
  // If this field is not set, all existing logs will be sent and then the new ones as
  // they appear. In essence it has 'tail -f' semantics.
  google.protobuf.Timestamp to_time = 4;

  // Record token. Set `record_token` to the `next_record_token` returned by a previous StreamLogs
  // request to start streaming from next log record.
  string record_token = 5 [(length) = "<=100", (sensitive) = false];

  // A filter expression that filters resources listed in the response.
  // The expression must specify:
  // 1. The field name. Currently filtering can be applied to the [LogRecord.logs.hostname] field.
  // 2. A conditional operator. Can be either `=` or `!=` for single values, `IN` or `NOT IN` for lists of values.
  // 3. The value. Must be 3-63 characters long and match the regular expression `^[a-z][-a-z0-9]{1,61}[a-z0-9]$`.
  // Examples of a filter: `message.hostname='node1.db.cloud.yandex.net'`
  string filter = 6 [(length) = "<=1000"];
}

message ListClusterOperationsRequest {
  // ID of the Apache Kafka Cluster resource to list operations for.
  string cluster_id = 1 [(required) = true, (length) = "<=50"];

  // The maximum number of results per page to return. If the number of available
  // results is larger than [page_size], the service returns a [ListClusterOperationsResponse.next_page_token]
  // that can be used to get the next page of results in subsequent list requests.
  int64 page_size = 2 [(value) = "<=1000"];

  // Page token.  To get the next page of results, set [page_token] to the [ListClusterOperationsResponse.next_page_token]
  // returned by a previous list request.
  string page_token = 3 [(length) = "<=100"];
}

message ListClusterOperationsResponse {
  // List of Operation resources for the specified Apache Kafka cluster.
  repeated operation.Operation operations = 1;

  // This token allows you to get the next page of results for list requests. If the number of results
  // is larger than [ListClusterOperationsRequest.page_size], use the [next_page_token] as the value
  // for the [ListClusterOperationsRequest.page_token] query parameter in the next list request.
  // Each subsequent list request will have its own [next_page_token] to continue paging through the results.
  string next_page_token = 2;
}

message ListClusterHostsRequest {
  // ID of the Apache Kafka cluster.
  // To get the Apache Kafka cluster ID use a [ClusterService.List] request.
  string cluster_id = 1 [(required) = true, (length) = "<=50"];

  // The maximum number of results per page to return. If the number of available
  // results is larger than [page_size], the service returns a [ListClusterHostsResponse.next_page_token]
  // that can be used to get the next page of results in subsequent list requests.
  int64 page_size = 2 [(value) = "<=1000"];

  // Page token.  To get the next page of results, set [page_token] to the [ListClusterHostsResponse.next_page_token]
  // returned by a previous list request.
  string page_token = 3 [(length) = "<=100"];
}

message ListClusterHostsResponse {
  // List of Host resources.
  repeated Host hosts = 1;

  // This token allows you to get the next page of results for list requests. If the number of results
  // is larger than [ListClusterHostsRequest.page_size], use the [next_page_token] as the value
  // for the [ListClusterHostsRequest.page_token] query parameter in the next list request.
  // Each subsequent list request will have its own [next_page_token] to continue paging through the results.
  string next_page_token = 2;
}

message MoveClusterRequest {
  // ID of the Apache Kafka cluster to move.
  string cluster_id = 1 [(required) = true, (length) = "<=50"];
  // ID of the destination folder.
  string destination_folder_id = 2 [(required) = true, (length) = "<=50"];
}

message MoveClusterMetadata {
  // ID of the Apache Kafka cluster being moved.
  string cluster_id = 1;
  // ID of the source folder.
  string source_folder_id = 2;
  // ID of the destnation folder.
  string destination_folder_id = 3;
}

message StartClusterRequest {
  // Required. ID of the Apache Kafka cluster to start.
  string cluster_id = 1 [(required) = true, (length) = "<=50"];
}

message StartClusterMetadata {
  // Required. ID of the Apache Kafka cluster.
  string cluster_id = 1;
}

message StopClusterRequest {
  // Required. ID of the Apache Kafka cluster to stop.
  string cluster_id = 1 [(required) = true, (length) = "<=50"];
}

message StopClusterMetadata {
  // Required. ID of the Apache Kafka cluster.
  string cluster_id = 1;
}

message RescheduleMaintenanceRequest {
  // Required. ID of the Kafka cluster to maintenance reschedule.
  string cluster_id = 1 [(required) = true, (length) = "<=50"];

  enum RescheduleType {
    RESCHEDULE_TYPE_UNSPECIFIED = 0;
    IMMEDIATE = 1;
    NEXT_AVAILABLE_WINDOW = 2;
    SPECIFIC_TIME = 3;
  }
  // Required. The type of reschedule request.
  RescheduleType reschedule_type = 2[(required) = true];

  // The time for SPECIFIC_TIME reschedule. Limited by two weeks since first time scheduled.
  google.protobuf.Timestamp delayed_until = 3;
}

message RescheduleMaintenanceMetadata {
  // Required. ID of the Kafka cluster.
  string cluster_id = 1;

  // Required. New time of the planned maintenance. Can be in the past for rescheduled to "IMMEDIATE".
  google.protobuf.Timestamp delayed_until = 4;
}
