syntax = "proto3";

package yandex.cloud.priv.datatransfer.v1;

option go_package = "a.yandex-team.ru/cloud/bitbucket/private-api/yandex/cloud/priv/datatransfer/v1;tm_server";

import "yandex/cloud/priv/datatransfer/v1/endpoint.proto";

enum TransferType {
    TRANSFER_TYPE_UNSPECIFIED = 0;
    // Snapshot and increment
    SNAPSHOT_AND_INCREMENT = 1;
    // Snapshot
    SNAPSHOT_ONLY = 2;
    // Increment
    INCREMENT_ONLY = 3;
}
enum TransferStatus {
    TRANSFER_STATUS_UNSPECIFIED = 0;
    CREATING = 1;
    CREATED = 2;
    RUNNING = 3;
    STOPPING = 4;
    STOPPED = 5;
    ERROR = 6;
    SNAPSHOTTING = 7;
    DONE = 8;
}
enum OperationType {
    OPERATION_TYPE_UNSPECIFIED = 0;
    ACTIVATE = 1;
    DEACTIVATE = 2;
    PAUSE = 3;
    REUPLOAD = 4;
    START = 5;
    RESTART = 6;
    CHECKSUM = 7;
}
enum YtRuntimeCluster {
    YT_RUNTIME_CLUSTER_UNSPECIFIED = 0;
    // ARNOLD
    YT_RUNTIME_CLUSTER_ARNOLD = 1;
    // BOHR
    YT_RUNTIME_CLUSTER_BOHR = 2;
    // FREUD
    YT_RUNTIME_CLUSTER_FREUD = 3;
    // HAHN
    YT_RUNTIME_CLUSTER_HAHN = 4;
    // HUME
    YT_RUNTIME_CLUSTER_HUME = 5;
    // LANDAU
    YT_RUNTIME_CLUSTER_LANDAU = 6;
    // SENECA-MAN
    YT_RUNTIME_CLUSTER_SENECA_MAN = 7;
    // SENECA-SAS
    YT_RUNTIME_CLUSTER_SENECA_SAS = 8;
    // SENECA-VLA
    YT_RUNTIME_CLUSTER_SENECA_VLA = 9;
    // VANGA
    YT_RUNTIME_CLUSTER_VANGA = 10;
}
enum RegularSnapshotScheduleInterval {
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_UNSPECIFIED = 0;
    // Every 5 min
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_5MIN = 1;
    // Every 15 min
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_15MIN = 2;
    // Every 30 min
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_30MIN = 3;
    // Every hour
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_HOUR = 4;
    // Every 2 hours
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_2HOUR = 5;
    // Every 3 hours
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_3HOUR = 6;
    // Every 6 hours
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_6HOUR = 7;
    // Every 8 hours
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_8HOUR = 8;
    // Every 12 hours
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_12HOUR = 9;
    // Every 24 hours
    REGULAR_SNAPSHOT_SCHEDULE_INTERVAL_DAY = 10;
}
enum Flavor {
    FLAVOR_UNSPECIFIED = 0;
    // Small
    SMALL = 1;
    // Medium
    MEDIUM = 2;
    // Large
    LARGE = 3;
}
enum MaskFunction {
    MASK_FUNCTION_UNSPECIFIED = 0;
    // Hash
    MASK_FUNTION_HASH = 1;
    // Snooper
    MASK_FUNTION_SNOOPER = 2;
}
enum EventType {
    EVENT_TYPE_UNSPECIFIED = 0;
    // Delete row
    EVENT_TYPE_DELETE = 1;
    // Truncate table
    EVENT_TYPE_TRUNCATE = 2;
    // Drop table
    EVENT_TYPE_DROP_TABLE = 3;
}
message Transfer {
    string id = 1;
    string folder_id = 2;
    string created_at = 3;
    string name = 4;
    string description = 5;
    map<string,string> labels = 6;
    Endpoint source = 7;
    Endpoint target = 8;
    Runtime runtime = 9;
    TransferStatus status = 10;
    Dashboard dashboard = 11;
    TransferType type = 12;
    double progress = 13;
    string owner_id = 14;
    string warning = 15;
    RegularSnapshot regular_snapshot = 16;
    Transformation transformation = 17;
}
message Dashboard {
    string id = 1;
    map<string,string> params = 2;
    string link = 3;
}
message LogLevel {
    enum Level {
        LEVEL_UNSPECIFIED = 0;
        TRACE = 1;
        DEBUG = 2;
        INFO = 3;
        WARN = 4;
        ERROR = 5;
        FATAL = 6;
    }
    Level level = 1;
}
message Runtime {
    oneof runtime {
        // Local run
        LocalRuntime local_runtime = 1;
        // YT
        YtRuntime yt_runtime = 2;
        // Multi YT
        MultiYtRuntime multi_yt_runtime = 3;
        // Yandex Cloud
        YcRuntime yc_runtime = 4;
        // Serverless runtime
        ServerlessRuntime serverless_runtime = 5;
        // Amazon Web Services EC2
        Ec2Runtime ec2_runtime = 6;
    }
}
message ServerlessRuntime {
    // Operations count
    // 
    // Operations count for transfer.
    int64 job_count = 2;
    // Minimum log level
    LogLevel.Level minimum_log_level = 7;
    // Copy settings
    ShardingUploadParams upload_shard_params = 8;
}
message LocalRuntime {
}
message YtRuntime {
    // YT Cluster
    // 
    // YT Cluster for transfer operations.
    YtRuntimeCluster yt_cluster = 6;
    // YT Pool
    // 
    // YT Pool for transfer operations. Leave blank if unsure.
    string pool = 4;
    // CPU
    // 
    // Guaranteed CPU for each operation.
    double cpu = 1;
    // RAM
    // 
    // Guaranteed RAM for each operation.
    string ram = 5;
    // Operations count
    // 
    // Operations count for transfer.
    int64 job_count = 2;
    // Copy settings
    ShardingUploadParams upload_shard_params = 8;
    // Minimum log level
    LogLevel.Level minimum_log_level = 7;
}
message ShardingUploadParams {
    // Jobs count
    // 
    // Instances count for copy.
    int64 job_count = 1;
    // Process count
    // 
    // Process count for copy inside single instance.
    int64 process_count = 2;
}
message RegularSnapshot {
    oneof mode {
        // Disabled
        RegularSnapshotDisabled disabled = 4;
        // Regular
        RegularSnapshotSettings settings = 3;
    }
}
message RegularSnapshotDisabled {
}
message RegularSnapshotSettings {
    // Interval
    // 
    // After that, a sync is run once the time since the last sync (whether it was
    // triggered manually or due to a schedule) has exceeded the schedule
    // interval.<br/> For example, consider the following illustrative
    // scenario:<br/>October 1st, 2pm, a user sets up a connection to sync data every
    // 24 hours.<br/>October 1st, 2:01pm: sync job runs<br/>October 2nd, 2:01pm: 24
    // hours have passed since the last sync, so a sync is triggered.<br/>October 2nd,
    // 5pm: The user manually triggers a sync from the UI<br/>October 3rd, 2:01pm:
    // since the last sync was less than 24 hours ago, no sync is run<br/>October 3rd,
    // 5:01pm: It has been more than 24 hours since the last sync, so a sync is run
    RegularSnapshotScheduleInterval schedule = 1;
    // Incremental Tables
    // 
    // List of tables that have cursor field for incremental snapshots. That is, it
    // only reads data that was generated or updated since the last time it ran, and is
    // thus far more efficient than a stream which reads all the source data every time
    // it runs.
    repeated IncrementalTable tables = 2;
}
message IncrementalTable {
    // Table Namespace
    // 
    // Data namespace  (e.g: db table schema)
    string table_namespace = 1;
    // Table Name
    // 
    // Table name (e.g: db table name)
    string table_name = 2;
    // Cursor column
    // 
    // The cursor_column refers to the field in the data output records used to
    // determine the "recency" or ordering of records. An example is a created_at or
    // updated_at field in an API or DB table. Cursor fields can be input by the user
    // (e.g: a user can choose to use an auto-incrementing id column in a DB table) or
    // they can be defined by the source e.g: where an API defines that updated_at is
    // what determines the ordering of records
    string cursor_column = 3;
}
message MultiYtRuntime {
    message YtDc {
        // YT Cluster
        // 
        // YT Cluster for transfer operations.
        YtRuntimeCluster yt_cluster = 6;
        // YT Pool
        // 
        // YT Pool for transfer operations. Leave blank if unsure.
        string pool = 4;
        // CPU
        // 
        // Guaranteed CPU for each operation.
        double cpu = 2;
        // RAM
        // 
        // Guaranteed RAM for each operation.
        string ram = 5;
        // Operations count
        // 
        // Operations count for transfer.
        int64 job_count = 1;
        // Minimum log level
        LogLevel.Level minimum_log_level = 7;
    }
    // YT Clusters
    // 
    // YT Clusters for transfer operations.
    repeated YtDc yts = 10;
}
message YcRuntime {
    // VMs flavor
    // 
    // The performance of the virtual machine, please choose a machine that matches
    // your data count.
    Flavor flavor = 2;
    // VMs count
    // 
    // Virtual machines count for run transfer.
    int64 job_count = 1;
    // Minimum log level
    LogLevel.Level minimum_log_level = 7;
    // Sharded copy properties
    // 
    // Properties of copying with multiple hosts. Multiple hosts usage increases
    // throughput of a transfer.
    ShardingUploadParams upload_shard_params = 8;
}
message Ec2Runtime {
    // VMs flavor
    // 
    // The performance of the virtual machine, please choose a machine that matches
    // your data count.
    Flavor flavor = 2;
    // Minimum log level
    LogLevel.Level minimum_log_level = 7;
    // Settings
    Settings settings = 8;
}
message Settings {
    oneof settings {
        // Automatic
        AutoSettings auto_settings = 1;
        // Manual
        ManualSettings manual_settings = 2;
    }
}
message AutoSettings {
}
message ManualSettings {
    // VM region
    string region = 1;
    // AWS Subnet Identifier
    string subnet_id = 2;
    // AWS Security Group Identifier
    string sg_id = 3;
}
message TablesFilter {
    // Include tables
    // 
    // Set as regular expression
    repeated string include_tables = 1;
    // Exclude tables
    // 
    // Set as regular expression
    repeated string exclude_tables = 2;
}
message ColumnsFilter {
    // Include columns
    // 
    // Set as regular expression
    repeated string include_columns = 1;
    // Exclude columns
    // 
    // Set as regular expression
    repeated string exclude_columns = 2;
}
message MaskFieldTransformer {
    // Tables
    TablesFilter tables = 1;
    // Columns list
    repeated string columns = 2;
    // Mask function
    MaskFunction function = 3;
}
message FilterColumnsTransformer {
    // Tables
    TablesFilter tables = 1;
    // Columns
    ColumnsFilter columns = 2;
}
message SkipEventsTransformer {
    // Tables
    TablesFilter tables = 1;
    repeated EventType events = 3;
}
message Transformer {
    oneof transformer {
        // Mask secrets fields
        MaskFieldTransformer mask_field = 1;
        // Columns filter
        FilterColumnsTransformer filter_columns = 2;
        // Skip events
        SkipEventsTransformer skip_events = 3;
    }
}
message ObjectStorage {
    // Bucket
    string bucket = 1;
    // SA Account
    string service_account_id = 8;
    // Access Key
    string access_key = 2;
}
message YDSTopic {
    // Database
    string database = 1;
    // Stream
    string stream = 2;
    // SA Account
    string service_account_id = 8;
}
message TransformerErrorsOutput {
    oneof output {
        // devnull
        bool devnull = 1;
        // Transfer destination
        bool transfer_destination = 2;
        // S3 bucket
        ObjectStorage bucket_id = 3;
        // YDS topic
        YDSTopic yds_topic = 4;
    }
}
message Transformation {
    // Debug mode
    bool debug_mode = 2;
    // Errors output
    TransformerErrorsOutput errors_output = 3;
    // Transformers list
    repeated Transformer transformers = 1;
}
