syntax = "proto3";

package yandex.cloud.priv.datatransfer.v1.endpoint.airbyte;

option go_package = "a.yandex-team.ru/cloud/bitbucket/private-api/yandex/cloud/priv/datatransfer/v1/endpoint/airbyte;tm_server_airbyte_endpoint";

import "yandex/cloud/priv/sensitive.proto";

message S3Source {
    message Format {
        oneof format {
            // csv
            // 
            // This connector utilises <a href="https: //
            // arrow.apache.org/docs/python/generated/pyarrow.csv.open_csv.html"
            // target="_blank">PyArrow (Apache Arrow)</a> for CSV parsing.
            Csv csv = 4;
            // parquet
            // 
            // This connector utilises <a
            // href="https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetFile.html"
            // target="_blank">PyArrow (Apache Arrow)</a> for Parquet parsing.
            Parquet parquet = 5;
        }
    }
    message Provider {
        // Bucket
        // 
        // Name of the S3 bucket where the file(s) exist.
        string bucket = 1;
        // Aws Access Key Id
        // 
        // In order to access private Buckets stored on AWS S3, this connector requires
        // credentials with the proper permissions. If accessing publicly available data,
        // this field is not necessary.
        string aws_access_key_id = 2 [(sensitive) = true];
        // Aws Secret Access Key
        // 
        // In order to access private Buckets stored on AWS S3, this connector requires
        // credentials with the proper permissions. If accessing publicly available data,
        // this field is not necessary.
        string aws_secret_access_key = 3 [(sensitive) = true];
        // Path Prefix
        // 
        // By providing a path-like prefix (e.g. myFolder/thisTable/) under which all the
        // relevant files sit, we can optimise finding these in S3. This is optional but
        // recommended if your bucket contains many folders/files.
        string path_prefix = 4;
        // Endpoint
        // 
        // Endpoint to an S3 compatible service. Leave empty to use AWS.
        string endpoint = 5;
        // Use Ssl
        // 
        // Is remote server using secure SSL/TLS connection
        bool use_ssl = 6;
        // Verify Ssl Cert
        // 
        // Allow self signed certificates
        bool verify_ssl_cert = 7;
    }
    message Csv {
        // Delimiter
        // 
        // The character delimiting individual cells in the CSV data. This may only be a
        // 1-character string.
        string delimiter = 2;
        // Quote Char
        // 
        // The character used optionally for quoting CSV values. To disallow quoting, make
        // this field blank.
        string quote_char = 3;
        // Escape Char
        // 
        // The character used optionally for escaping special characters. To disallow
        // escaping, leave this field blank.
        string escape_char = 4;
        // Encoding
        // 
        // The character encoding of the CSV data. Leave blank to default to
        // <strong>UTF-8</strong>. See <a
        // href="https://docs.python.org/3/library/codecs.html#standard-encodings"
        // target="_blank">list of python encodings</a> for allowable options.
        string encoding = 5;
        // Double Quote
        // 
        // Whether two quotes in a quoted CSV value denote a single quote in the data.
        bool double_quote = 6;
        // Newlines In Values
        // 
        // Whether newline characters are allowed in CSV values. Turning this on may affect
        // performance. Leave blank to default to False.
        bool newlines_in_values = 7;
        // Block Size
        // 
        // The chunk size in bytes to process at a time in memory from each file. If your
        // data is particularly wide and failing during schema detection, increasing this
        // should solve it. Beware of raising this too high as you could hit OOM errors.
        int64 block_size = 8;
        // Additional Reader Options
        // 
        // Optionally add a valid JSON string here to provide additional options to the csv
        // reader. Mappings must correspond to options <a
        // href="https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html#pyarrow.csv.ConvertOptions"
        // target="_blank">detailed here</a>. column_types is used internally to handle
        // schema so overriding that would likely cause problems. Example:
        // {"timestamp_parsers": ["%m/%d/%Y %H:%M", "%Y/%m/%d %H:%M"],
        // "strings_can_be_null": true, "null_values": ["NA", "NULL"]}
        string additional_reader_options = 9;
        // Advanced Options
        // 
        // Optionally add a valid JSON string here to provide additional <a
        // href="https://arrow.apache.org/docs/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions"
        // target="_blank">Pyarrow ReadOptions</a>. Specify column_names here if your CSV
        // doesnt have header, or if you want to use custom column names. block_size and
        // encoding are already used above, specify them again here will override the
        // values above.
        string advanced_options = 10;
    }
    message Parquet {
        // Buffer Size
        // 
        // Perform read buffering when deserializing individual column chunks. By default
        // every group column will be loaded fully to memory. This option can help to
        // optimize a work with memory if your data is particularly wide or failing during
        // detection of OOM errors.
        int64 buffer_size = 2;
        // Columns
        // 
        // If you only want to sync a subset of the columns from the file(s), add the
        // columns you want here. Leave it empty to sync all columns.
        repeated string columns = 3;
        // Batch Size
        // 
        // Maximum number of records per batch. Batches may be smaller if there are not
        // enough rows in the file. This option can help to optimize a work with memory if
        // your data is particularly wide or failing during detection of OOM errors.
        int64 batch_size = 4;
    }
    // Dataset
    // 
    // This source creates one table per connection, this field is the name of that
    // table. This should include only letters, numbers, dash and underscores. Note
    // that this may be altered according to destination.
    string dataset = 1;
    // Path Pattern
    // 
    // Add at least 1 pattern here to match filepaths against. Use | to separate
    // multiple patterns. Airbyte uses these patterns to determine which files to pick
    // up from the provider storage. See <a
    // href="https://facelessuser.github.io/wcmatch/glob/"
    // target="_blank">wcmatch.glob</a> to understand pattern syntax (GLOBSTAR and
    // SPLIT flags are enabled). Use pattern <strong>**</strong> to pick up all files.
    // Examples: "**" "myFolder/myTableFiles/*.csv|myFolder/myOtherTableFiles/*.csv"
    string path_pattern = 2;
    // Schema
    // 
    // Optionally provide a schema to enforce, as a valid JSON string. Ensure this is a
    // mapping of <strong>{ "column" : "type" }</strong>, where types are valid <a
    // href="https://json-schema.org/understanding-json-schema/reference/type.html"
    // target="_blank">JSON Schema datatypes</a>. Leave as {} to auto-infer the schema.
    // Example: {"column_1": "number", "column_2": "string", "column_3": "array",
    // "column_4": "object", "column_5": "boolean"}
    string schema = 3;
    Format format = 5;
    // S3: Amazon Web Services
    Provider provider = 6;
}
